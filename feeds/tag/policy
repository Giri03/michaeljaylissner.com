<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Michael Jay Lissner</title><link href="http://michaeljaylissner.com/" rel="alternate"></link><link href="http://michaeljaylissner.com/feeds/tag/policy" rel="self"></link><id>http://michaeljaylissner.com/</id><updated>2012-04-27T17:05:21+02:00</updated><entry><title>Further privacy protections at CourtListener</title><link href="http://michaeljaylissner.com/posts/2012/04/27/further-privacy-protections-at-courtlistener/" rel="alternate"></link><updated>2012-04-27T17:05:21+02:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2012-04-27:posts/2012/04/27/further-privacy-protections-at-courtlistener/</id><summary type="html">&lt;p&gt;I've &lt;a href="http://michaeljaylissner.com/blog/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents"&gt;written previously&lt;/a&gt; about the lengths we go to at CourtListener to protect people's privacy, and today we completed one more privacy enhancement. &lt;/p&gt;
&lt;p&gt;After my last post on this topic, we discovered that although we had already blocked cases from appearing in the search results of all major search engines, we had a privacy leak in the form of our computer-readable sitemaps. These sitemaps contain links to every page within a website, and since those links contain the names of the parties in a case, it's possible that a Google search for the party name could turn up results that should be hidden.&lt;/p&gt;
&lt;p&gt;This was problematic, and as of now we have changed the way we serve sitemaps so that they use the noindex X-Robots-Tag HTTP header. This tells search crawlers that they are welcome to read our sitemaps, but that they should avoid serving them or indexing them.&lt;/p&gt;</summary><category term="sitemaps.xml"></category><category term="privacy"></category><category term="policy"></category><category term="courtlistener.com"></category></entry><entry><title>Respecting privacy while providing hundreds of thousands of public documents</title><link href="http://michaeljaylissner.com/posts/2012/01/16/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents/" rel="alternate"></link><updated>2012-01-16T22:13:22+01:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2012-01-16:posts/2012/01/16/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents/</id><summary type="html">&lt;p&gt;At CourtListener, we have always taken privacy very seriously. We &lt;a href="http://courtlistener.com/coverage/"&gt;have over 600,000&lt;/a&gt; cases currently, most of which are available on Google and other search engines. But in the interest of privacy, we make two broad exceptions to what's available on search engines: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As is stated in our &lt;a href="http://courtlistener.com/removal/"&gt;removal policy&lt;/a&gt;, if someone gets in touch with us in writing and requests that we block search engines from indexing a document, we generally attempt to do so within a few hours. &lt;/li&gt;
&lt;li&gt;If we discover a privacy problem within a case, we proactively block search engines from indexing it. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these exceptions presents interesting problems. In the case of requests to prevent indexing by search engines, we're often faced with an ethical dilemma, since in many instances, the party making the request is merely displeased that their involvement in the case is easy to discover and/or they are simply embarrassed by their past. In this case, the question we have to ask ourselves is: Where is the balance between the person's right to privacy and the public's need to access court records, and to what extent do changes in &lt;a href="http://scholar.google.com/scholar?hl=en&amp;amp;q=practical+obscurity+privacy"&gt;practical obscurity&lt;/a&gt; compel action on our behalf? For example, if someone convicted of murder or child molestation is trying to make information about their past harder to discover, how should we weigh the public's interest in easily locating this information via a search engine? In the case of convicted child molesters, we can look to &lt;a href="http://en.wikipedia.org/wiki/Megan%27s_Law"&gt;Megan's law&lt;/a&gt; for a public policy stance on the issue, but even that forces us to ask to what extent we should chart our own path, and to what extent we should follow public policy decisions. &lt;/p&gt;
&lt;p&gt;On the opposite end of the spectrum, many of the cases that we block search engines from indexing are asylum cases where a person has lost an attempt to stay in the United States, and been sent back to a country where they feel unsafe. In such cases, it seems clear that it's important to keep the person's name out of search engine results, but still we must ask to what extent do we have an obligation to identify and block such cases from appearing proactively rather than post hoc? &lt;/p&gt;
&lt;p&gt;In both of these scenarios, we have taken a middle ground that we hope strikes a balance between the public's need for court documents and an individual's desire or need for privacy. Instead of either proactively blocking search engines from indexing cases or keeping cases in search results against a party's request, our current policy is to block search engines from indexing a web page as each request comes in. We currently have 190 cases that are blocked from search results, and the number increases regularly. &lt;/p&gt;
&lt;p&gt;Where we do take proactive measures to block cases from search results is where we have discovered unredacted or &lt;a href="https://freedom-to-tinker.com/blog/tblee/what-gets-redacted-pacer"&gt;improperly redacted&lt;/a&gt; social security numbers in a case. Taking a cue from the now-defunct Altlaw, whenever a case is added, we look for character strings that appear to be social security numbers, tax ID numbers or alien ID numbers. If we find any such strings, we replace them with x's, and we try to make sure the unredacted document does not appear in search results outside of CourtListener. &lt;/p&gt;
&lt;p&gt;The methods we have used to block cases from appearing in search results have evolved over time, and I'd like to share what we've learned so others can give us feedback and learn from our experiences. There are five technical measures we use to keep a case out of search results: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;robots.txt file &lt;/li&gt;
&lt;li&gt;HTML meta noindex tags &lt;/li&gt;
&lt;li&gt;HTTP X-Robots-Tag headers &lt;/li&gt;
&lt;li&gt;sitemaps.xml files &lt;/li&gt;
&lt;li&gt;The webmaster tools provided by the search engines themselves&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these deserves a moment of explanation. &lt;a href="http://www.robotstxt.org/"&gt;robots.txt&lt;/a&gt; is a protocol that is respected by all major search engines internationally, and which allows site authors (such as myself) to identify web pages that shouldn't be crawled. Note that I said &lt;strong&gt;crawled&lt;/strong&gt; not &lt;strong&gt;indexed&lt;/strong&gt;. This is a very important distinction, as I'll explain momentarily. &lt;/p&gt;
&lt;p&gt;HTML meta tags are a tag that you can place into the HTML of a page, and which instructs search engines not to &lt;strong&gt;index&lt;/strong&gt; a page. Since this is an HTML format, this method only works on HTML pages. &lt;/p&gt;
&lt;p&gt;HTTP X-Robots-Tag headers are similar to HTML meta tags, but they allow site authors to request that an &lt;em&gt;item&lt;/em&gt; not be indexed. That item may be an HTML page, but equally, it may be a PDF or even an image that should not searchable. &lt;/p&gt;
&lt;p&gt;Further, we provide an &lt;a href="http://www.sitemaps.org/protocol.html"&gt;XML sitemap&lt;/a&gt; that search engines can understand, and which tells them about every page on the site that they should crawl and index. &lt;/p&gt;
&lt;p&gt;All of these elements fit together into a complicated melange that has 
absorbed many development hours over the past two years, 
as different search engines interpret these standards in different ways. &lt;/p&gt;
&lt;p&gt;For example, Google and Bing interpret the robots.txt files as blocks to 
their crawlers. This means that web pages listed in robots.txt will not be 
&lt;strong&gt;crawled&lt;/strong&gt; by Google or Bing, but that does not mean those pages will not 
be &lt;strong&gt;indexed&lt;/strong&gt;. Indeed, if Google or Bing learn of the existence of a web 
page (for example, because another page linked to it), 
then they will include it in &lt;a href="http://www.youtube.com/watch?v=KBdEwpRQRD0"&gt;their&lt;/a&gt; &lt;a href="http://www.bing.com/community/site_blogs/b/webmaster/archive/2009/08/21/prevent-a-bot-from-getting-lost-in-space-sem-101.aspx"&gt;indexes&lt;/a&gt;. This is true even if 
robots.txt explicitly blocks robots from crawling the page, 
because to include it in their indexes, they don't &lt;strong&gt;have to&lt;/strong&gt; crawl it 
&amp;mdash; they just need to know about it! Even your own link to a page is 
sufficient for Google or Bing to know about the page. And what's worse, 
if you have a good URL with descriptive words within it, 
Google or Bing will know the terms in the URLs even when they haven't 
crawled the page. So if your URL is example
.com/private-page-about-michael-jackson, a query for [ Michael Jackson ] 
could certainly bring it up, even if it were never crawled. &lt;/p&gt;
&lt;p&gt;The solution to this is to allow Google and Bing to crawl the pages, but to use noindex meta or HTTP tags. If these are in place, the pages will not appear in the index at all. This sounds paradoxical: to exclude pages from appearing in Google and Bing, you have to allow them to be crawled? &lt;a href="https://support.google.com/webmasters/bin/answer.py?hl=en&amp;amp;answer=93710"&gt;Yes, that's correct&lt;/a&gt;. Furthermore, it's theoretically possible that Google or Bing could learn about a page on your site from a link, and then not crawl it immediately or at all. In this case, they will know the URL, but won't know about and X-Robots-Tag headers or meta tags. Thus, they might include the document against your wishes. For this reason, it's important to &lt;strong&gt;include&lt;/strong&gt; private pages in your sitemap.xml file, inviting and encouraging Google and Bing to crawl the page specifically so the page can be excluded from their indexes.&lt;/p&gt;
&lt;p&gt;Yahoo! uses Bing to power their search engine, and AOL uses Google, so the above strategy applies to them as well.&lt;/p&gt;
&lt;p&gt;Other search engines take a different approach to robots.txt. Ask.com, The Internet Archive and the Russian search engine Yandex.ru all respect the robots meta tag, but not the x-robots-tag HTTP header. Thus, for these search engines, the strategy above works for HTML files, but not for any other files. These crawlers therefore need to be blocked from accessing those other files. On the upside, unlike Google and Bing, it appears that these search engines will not show a document in their results if they have not crawled it. Thus, using robots.txt alone should be sufficient.&lt;/p&gt;
&lt;p&gt;A third class of search engines support neither the robots HTML meta tag, nor the x-robots-tag HTTP header. These are typically less popular or less mature crawlers, and so they must be blocked using robots.txt. There are two approaches to this. The first is to list blocked pages individually in the robots.txt file, and the second is to simply block these search engines from all access. While it's possible to list each private document in robots.txt, doing so creates a privacy loophole, since it lists all private documents in one place. At CourtListener, therefore, we take a conservative approach, and completely block all search engines that do not support the HTML meta tag or the x-robots-tag HTTP header.&lt;/p&gt;
&lt;p&gt;The final action we take when we receive a request that a document on our site stop appearring in search results, is to use the webmaster tools provided by the major search engines&lt;sup&gt;1&lt;/sup&gt; to explicitly ask those search engines to exclude the document(s) from their results.&lt;/p&gt;
&lt;p&gt;Between these measures, private documents on CourtListener should be removed from all major and minor search engines. Where posssible this strategy takes a very granular approach, and where minor search engines do not support certain standards, we take a conservative approach, blocking them entirely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update, 2012-04-29:&lt;/strong&gt; You may also want to look at our &lt;a href="http://michaeljaylissner.com/blog/further-privacy-protections-at-courtlistener"&gt;discussion of the impact of putting people's names into your URLs, and the way that affects your sitemap files&lt;/a&gt;.&lt;/p&gt;
&lt;!-- actual footnotes --&gt;

&lt;ol&gt;
&lt;li&gt;We use &lt;a href="http://www.google.com/webmasters/tools"&gt;Google's Webmaster 
Tools&lt;/a&gt; and &lt;a href="http://www.bing.com/toolbox/webmaster"&gt;Bing's 
Webmaster Tools&lt;/a&gt;. Before it was merged into Bing's tools, 
we also previously used &lt;a href="http://siteexplorer.search.yahoo
.com/"&gt;Yahoo's Site Explorer&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;</summary><category term="robots.txt"></category><category term="privacy"></category><category term="practical obscurity"></category><category term="policy"></category><category term="courtlistener.com"></category><category term="courtlistener.com"></category><category term="policy"></category><category term="practical obscurity"></category><category term="privacy"></category><category term="robots.txt"></category></entry><entry><title>Respecting privacy while providing hundreds of thousands of public documents</title><link href="http://michaeljaylissner.com/posts/2012/01/16/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents/" rel="alternate"></link><updated>2012-01-16T22:13:22+01:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2012-01-16:posts/2012/01/16/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents/</id><summary type="html">&lt;p&gt;At CourtListener, we have always taken privacy very seriously. We &lt;a href="http://courtlistener.com/coverage/"&gt;have over 600,000&lt;/a&gt; cases currently, most of which are available on Google and other search engines. But in the interest of privacy, we make two broad exceptions to what's available on search engines: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As is stated in our &lt;a href="http://courtlistener.com/removal/"&gt;removal policy&lt;/a&gt;, if someone gets in touch with us in writing and requests that we block search engines from indexing a document, we generally attempt to do so within a few hours. &lt;/li&gt;
&lt;li&gt;If we discover a privacy problem within a case, we proactively block search engines from indexing it. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these exceptions presents interesting problems. In the case of requests to prevent indexing by search engines, we're often faced with an ethical dilemma, since in many instances, the party making the request is merely displeased that their involvement in the case is easy to discover and/or they are simply embarrassed by their past. In this case, the question we have to ask ourselves is: Where is the balance between the person's right to privacy and the public's need to access court records, and to what extent do changes in &lt;a href="http://scholar.google.com/scholar?hl=en&amp;amp;q=practical+obscurity+privacy"&gt;practical obscurity&lt;/a&gt; compel action on our behalf? For example, if someone convicted of murder or child molestation is trying to make information about their past harder to discover, how should we weigh the public's interest in easily locating this information via a search engine? In the case of convicted child molesters, we can look to &lt;a href="http://en.wikipedia.org/wiki/Megan%27s_Law"&gt;Megan's law&lt;/a&gt; for a public policy stance on the issue, but even that forces us to ask to what extent we should chart our own path, and to what extent we should follow public policy decisions. &lt;/p&gt;
&lt;p&gt;On the opposite end of the spectrum, many of the cases that we block search engines from indexing are asylum cases where a person has lost an attempt to stay in the United States, and been sent back to a country where they feel unsafe. In such cases, it seems clear that it's important to keep the person's name out of search engine results, but still we must ask to what extent do we have an obligation to identify and block such cases from appearing proactively rather than post hoc? &lt;/p&gt;
&lt;p&gt;In both of these scenarios, we have taken a middle ground that we hope strikes a balance between the public's need for court documents and an individual's desire or need for privacy. Instead of either proactively blocking search engines from indexing cases or keeping cases in search results against a party's request, our current policy is to block search engines from indexing a web page as each request comes in. We currently have 190 cases that are blocked from search results, and the number increases regularly. &lt;/p&gt;
&lt;p&gt;Where we do take proactive measures to block cases from search results is where we have discovered unredacted or &lt;a href="https://freedom-to-tinker.com/blog/tblee/what-gets-redacted-pacer"&gt;improperly redacted&lt;/a&gt; social security numbers in a case. Taking a cue from the now-defunct Altlaw, whenever a case is added, we look for character strings that appear to be social security numbers, tax ID numbers or alien ID numbers. If we find any such strings, we replace them with x's, and we try to make sure the unredacted document does not appear in search results outside of CourtListener. &lt;/p&gt;
&lt;p&gt;The methods we have used to block cases from appearing in search results have evolved over time, and I'd like to share what we've learned so others can give us feedback and learn from our experiences. There are five technical measures we use to keep a case out of search results: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;robots.txt file &lt;/li&gt;
&lt;li&gt;HTML meta noindex tags &lt;/li&gt;
&lt;li&gt;HTTP X-Robots-Tag headers &lt;/li&gt;
&lt;li&gt;sitemaps.xml files &lt;/li&gt;
&lt;li&gt;The webmaster tools provided by the search engines themselves&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these deserves a moment of explanation. &lt;a href="http://www.robotstxt.org/"&gt;robots.txt&lt;/a&gt; is a protocol that is respected by all major search engines internationally, and which allows site authors (such as myself) to identify web pages that shouldn't be crawled. Note that I said &lt;strong&gt;crawled&lt;/strong&gt; not &lt;strong&gt;indexed&lt;/strong&gt;. This is a very important distinction, as I'll explain momentarily. &lt;/p&gt;
&lt;p&gt;HTML meta tags are a tag that you can place into the HTML of a page, and which instructs search engines not to &lt;strong&gt;index&lt;/strong&gt; a page. Since this is an HTML format, this method only works on HTML pages. &lt;/p&gt;
&lt;p&gt;HTTP X-Robots-Tag headers are similar to HTML meta tags, but they allow site authors to request that an &lt;em&gt;item&lt;/em&gt; not be indexed. That item may be an HTML page, but equally, it may be a PDF or even an image that should not searchable. &lt;/p&gt;
&lt;p&gt;Further, we provide an &lt;a href="http://www.sitemaps.org/protocol.html"&gt;XML sitemap&lt;/a&gt; that search engines can understand, and which tells them about every page on the site that they should crawl and index. &lt;/p&gt;
&lt;p&gt;All of these elements fit together into a complicated melange that has 
absorbed many development hours over the past two years, 
as different search engines interpret these standards in different ways. &lt;/p&gt;
&lt;p&gt;For example, Google and Bing interpret the robots.txt files as blocks to 
their crawlers. This means that web pages listed in robots.txt will not be 
&lt;strong&gt;crawled&lt;/strong&gt; by Google or Bing, but that does not mean those pages will not 
be &lt;strong&gt;indexed&lt;/strong&gt;. Indeed, if Google or Bing learn of the existence of a web 
page (for example, because another page linked to it), 
then they will include it in &lt;a href="http://www.youtube.com/watch?v=KBdEwpRQRD0"&gt;their&lt;/a&gt; &lt;a href="http://www.bing.com/community/site_blogs/b/webmaster/archive/2009/08/21/prevent-a-bot-from-getting-lost-in-space-sem-101.aspx"&gt;indexes&lt;/a&gt;. This is true even if 
robots.txt explicitly blocks robots from crawling the page, 
because to include it in their indexes, they don't &lt;strong&gt;have to&lt;/strong&gt; crawl it 
&amp;mdash; they just need to know about it! Even your own link to a page is 
sufficient for Google or Bing to know about the page. And what's worse, 
if you have a good URL with descriptive words within it, 
Google or Bing will know the terms in the URLs even when they haven't 
crawled the page. So if your URL is example
.com/private-page-about-michael-jackson, a query for [ Michael Jackson ] 
could certainly bring it up, even if it were never crawled. &lt;/p&gt;
&lt;p&gt;The solution to this is to allow Google and Bing to crawl the pages, but to use noindex meta or HTTP tags. If these are in place, the pages will not appear in the index at all. This sounds paradoxical: to exclude pages from appearing in Google and Bing, you have to allow them to be crawled? &lt;a href="https://support.google.com/webmasters/bin/answer.py?hl=en&amp;amp;answer=93710"&gt;Yes, that's correct&lt;/a&gt;. Furthermore, it's theoretically possible that Google or Bing could learn about a page on your site from a link, and then not crawl it immediately or at all. In this case, they will know the URL, but won't know about and X-Robots-Tag headers or meta tags. Thus, they might include the document against your wishes. For this reason, it's important to &lt;strong&gt;include&lt;/strong&gt; private pages in your sitemap.xml file, inviting and encouraging Google and Bing to crawl the page specifically so the page can be excluded from their indexes.&lt;/p&gt;
&lt;p&gt;Yahoo! uses Bing to power their search engine, and AOL uses Google, so the above strategy applies to them as well.&lt;/p&gt;
&lt;p&gt;Other search engines take a different approach to robots.txt. Ask.com, The Internet Archive and the Russian search engine Yandex.ru all respect the robots meta tag, but not the x-robots-tag HTTP header. Thus, for these search engines, the strategy above works for HTML files, but not for any other files. These crawlers therefore need to be blocked from accessing those other files. On the upside, unlike Google and Bing, it appears that these search engines will not show a document in their results if they have not crawled it. Thus, using robots.txt alone should be sufficient.&lt;/p&gt;
&lt;p&gt;A third class of search engines support neither the robots HTML meta tag, nor the x-robots-tag HTTP header. These are typically less popular or less mature crawlers, and so they must be blocked using robots.txt. There are two approaches to this. The first is to list blocked pages individually in the robots.txt file, and the second is to simply block these search engines from all access. While it's possible to list each private document in robots.txt, doing so creates a privacy loophole, since it lists all private documents in one place. At CourtListener, therefore, we take a conservative approach, and completely block all search engines that do not support the HTML meta tag or the x-robots-tag HTTP header.&lt;/p&gt;
&lt;p&gt;The final action we take when we receive a request that a document on our site stop appearring in search results, is to use the webmaster tools provided by the major search engines&lt;sup&gt;1&lt;/sup&gt; to explicitly ask those search engines to exclude the document(s) from their results.&lt;/p&gt;
&lt;p&gt;Between these measures, private documents on CourtListener should be removed from all major and minor search engines. Where posssible this strategy takes a very granular approach, and where minor search engines do not support certain standards, we take a conservative approach, blocking them entirely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update, 2012-04-29:&lt;/strong&gt; You may also want to look at our &lt;a href="http://michaeljaylissner.com/blog/further-privacy-protections-at-courtlistener"&gt;discussion of the impact of putting people's names into your URLs, and the way that affects your sitemap files&lt;/a&gt;.&lt;/p&gt;
&lt;!-- actual footnotes --&gt;

&lt;ol&gt;
&lt;li&gt;We use &lt;a href="http://www.google.com/webmasters/tools"&gt;Google's Webmaster 
Tools&lt;/a&gt; and &lt;a href="http://www.bing.com/toolbox/webmaster"&gt;Bing's 
Webmaster Tools&lt;/a&gt;. Before it was merged into Bing's tools, 
we also previously used &lt;a href="http://siteexplorer.search.yahoo
.com/"&gt;Yahoo's Site Explorer&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;</summary><category term="robots.txt"></category><category term="privacy"></category><category term="practical obscurity"></category><category term="policy"></category><category term="courtlistener.com"></category><category term="courtlistener.com"></category><category term="policy"></category><category term="practical obscurity"></category><category term="privacy"></category><category term="robots.txt"></category></entry><entry><title>Project Idea: "Bug Trackers for Cities."</title><link href="http://michaeljaylissner.com/posts/2010/06/22/project-idea-bug-tracker-for-cities/" rel="alternate"></link><updated>2010-06-22T16:59:02+02:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2010-06-22:posts/2010/06/22/project-idea-bug-tracker-for-cities/</id><summary type="html">&lt;p&gt;Well, today's project idea was to post about the use of bug trackers for the management of city problems, but as it should turn out, I'm behind the curve on this one, so I'll just explain the concept, and post some links to people that have live implementations or have already blogged about this. When I first researched this idea about six months ago, I didn't find anything, but it seems that steam is building behind this idea.&lt;/p&gt;
&lt;p&gt;Essentially, the idea is this: Cities have problems that citizens know about such as potholes, busted lampposts, gang activity, etc. They want to report these things to the city, but unfortunately reporting the problems by the phone or navigating the city websites is usually an awful, time-consuming, and unrewarding experience. It goes like this: First you get bumped from one department to another, eventually finding somebody who seems like they care. You tell them about the problem and feel satisfied that you've done your part, but you don't know if it's really in their system, or when it's going to get fixed or anything. You hang up the phone, and the problem is still a part of your daily life. You know if you call again, you won't be able to get an update, and you resign yourself to simply &lt;em&gt;hoping&lt;/em&gt; that the problem will eventually be resolved. The next time you notice something that's in need of fixing, you're less likely to try to help. As this goes on, eventually the people that once cared no longer do, and getting residents of a city engaged in the problems in their community becomes increasingly difficult.&lt;/p&gt;
&lt;p&gt;In the software world, there is a similar phenomenon, except instead of infrastructure and safety problems, the problems are errors in the software that need to be fixed &amp;ndash; bugs. The solution to getting these bugs triaged and managed is to use what's known as a &lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/Bug_tracking_system" target="_blank"&gt;bug tracker&lt;/a&gt;. These systems allow the programmers behind the software to respond to problems that people find, and to triage them appropriately. In addition, they allow other people to vote on bugs, and help solve them. They allow careful prioritization of the bugs, and they allow visualizations of the bugs to be created such as the speed that they are fixed by department, the oldest bug in the system, etc.&lt;/p&gt;
&lt;p&gt;If such as system were used for citizens to track problems they find in their city, it would have all kinds of benefits, and indeed a few such systems have been created. The most popular that I have found is called &lt;a href="http://seeclickfix.com/" target="_blank"&gt;SeeClickFix&lt;/a&gt;, and looking at &lt;a href="http://seeclickfix.com/berkeley" target="_blank"&gt;the page for Berkeley&lt;/a&gt;, it seems like it is a system that is at least used by Berkeley residents. Another popular one is &lt;a href="http://www.fixmystreet.com/" target="_blank"&gt;http://www.fixmystreet.com/&lt;/a&gt;. Of course, for the system to be truly effective, it would have to be endorsed by the city itself, and used by its employees as well, which is something I have yet to find an example of.&lt;/p&gt;
&lt;p&gt;Other people have also &lt;a href="http://speedbird.wordpress.com/2010/04/22/frameworks-for-citizen-responsiveness/" target="_blank"&gt;written about this idea&lt;/a&gt;, and &lt;a href="http://portlandwiki.org/CivicApps#Notes_from_CivicApps_Meetup_at_Open_Source_Bridge_2010" target="_blank"&gt;Portland appears to be considering it&lt;/a&gt;, so it seems this idea is ripe on the vine and ready to be picked. &lt;/p&gt;
&lt;p&gt;The question now is what will it take to implement it correctly, and what system will be the one that gains usage. I fully expect to see more cities using this type of technology in the next few years. &lt;/p&gt;</summary><category term="urban development"></category><category term="policy"></category><category term="bugs"></category><category term="Project idea"></category></entry><entry><title>An Analysis of FTC Behavioral Advertising and an End of Semester Countdown</title><link href="http://michaeljaylissner.com/posts/2009/04/02/analysis-of-ftc-behavioral-advertising/" rel="alternate"></link><updated>2009-04-02T14:44:40+02:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2009-04-02:posts/2009/04/02/analysis-of-ftc-behavioral-advertising/</id><summary type="html">&lt;p&gt;It's coming down to the end of the semester, and after I finished the attached paper on FTC laws as the apply to online advertising, I did some calculations to figure out what I have to do still. &lt;/p&gt;
&lt;p&gt;Turns out I have 68-95 pages to write (give or take), and two projects to complete between today and early May. Things are going to get interesting. &lt;/p&gt;
&lt;p&gt;The lay of the land looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two law/policy papers - total of 35-50 pages&lt;/li&gt;
&lt;li&gt;One sociology paper - 25-35 pages&lt;/li&gt;
&lt;li&gt;A final project combining some aesthetics work I have been doing&lt;/li&gt;
&lt;li&gt;Two technology strategy assessments - total of eight pages&lt;/li&gt;
&lt;li&gt;And an online project - watch for this soon&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now, I'll reserve my thoughts on the attached analysis, but I tried to analyze the ways that the FTC regulates online advertising...within an eight page limit. &lt;/p&gt;</summary><category term="principles"></category><category term="policy"></category><category term="paper"></category><category term="law"></category><category term="FTC"></category><category term="advertising"></category></entry><entry><title>Rebuilding the 9th Ward in New Orleans</title><link href="http://michaeljaylissner.com/posts/2008/11/30/rebuilding-the-9th-ward-in-new-orleans/" rel="alternate"></link><updated>2008-11-30T20:11:06+01:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2008-11-30:posts/2008/11/30/rebuilding-the-9th-ward-in-new-orleans/</id><summary type="html">&lt;p&gt;I've spent Thanksgiving in the Deep South this year at my girlfriend's 
grandparent's house. It's been a great trip, but one thing about it keeps 
plaguing me.&lt;/p&gt;
&lt;p&gt;We spent a couple of days in New Orleans, and one of the places I insisted upon 
seeing was the 9th Ward, where Katrina dealt some of the worst damage. I wanted 
to see if people were recovering, and how much the place had been fixed up 
since 2005 when the storm blew through.&lt;/p&gt;
&lt;p&gt;What surprised me (aside from the &lt;a href="http://www.flickr.com/photos/bisset_linda/3026355210/"&gt;markings&lt;/a&gt;) is that people are moving 
back into this area, rebuilding their houses, and generally, pouring money into 
the location. On the one hand, it makes sense since it's their home, but on the 
other, I can't help but think that &lt;i&gt;&lt;strong&gt;somebody&lt;/strong&gt;&lt;/i&gt; should stop 
this from happening. From what I've learned while here, the 9th Ward is an area 
of New Orleans that is going to flood if there is another hurricane 
Katrina...which there will be sooner or later. Considering that Katrina 
&lt;a href="http://en.wikipedia.org/wiki/Hurricane_Katrina"&gt;killed almost 2,000 
people&lt;/a&gt; this seems rather unwise.&lt;/p&gt;
&lt;p&gt;So, rather than moving back in, shouldn't we be cutting our losses right about 
now, finding somewhere for these people to live, and preventing another 
disaster? Or is my logic flawed?&lt;/p&gt;</summary><category term="policy"></category><category term="katrina"></category><category term="disaster"></category><category term="bad ideas"></category></entry><entry><title>A Music Cost Inventory</title><link href="http://michaeljaylissner.com/posts/2008/11/22/a-music-cost-inventory/" rel="alternate"></link><updated>2008-11-22T17:21:02+01:00</updated><author><name>Michael Lissner</name></author><id>tag:michaeljaylissner.com,2008-11-22:posts/2008/11/22/a-music-cost-inventory/</id><summary type="html">&lt;p&gt;According to &lt;a href="http://www.copyright.gov/title17/92chap5.html" target="_blank"&gt;Title 17, Chapter 5, section 504c2 of the US copyright law&lt;/a&gt;, if you get caught with music that you have downloaded illegally from the Internet, you can get charged up to $150,000 per infringement. I thought I would do a little experiment to see how much I would be in for if my entire collection were to be found to be illegal. &lt;/p&gt;
&lt;p&gt;Let's do some math. I have 3,876 tracks, at $150,000 each. So if my entire collection were to be found illegal, that means it would cost me $581.4 million dollars &amp;mdash; about .6 billion dollars. &lt;/p&gt;
&lt;p&gt;OK, let's assume that I can live with that reality. It just seems odd that I could have bought those songs for $3,876 on amazon.com, or iTunes.&lt;/p&gt;
&lt;p&gt;Something isn't quite right here. Also, did I mention that all US digital music sales &lt;a href="http://www.ifpi.org/content/section_resources/dmr2008.html"&gt;are estimated&lt;/a&gt; to total $2.9B in 2007? That makes my music worth about 20% of the 2007 revenue.&lt;/p&gt;</summary><category term="music"></category><category term="copyright"></category><category term="IP"></category><category term="policy"></category><category term="law"></category></entry></feed>