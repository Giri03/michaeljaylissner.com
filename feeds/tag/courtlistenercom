<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Michael Jay Lissner</title><link href="http://michaeljaylissner.com/" rel="alternate"></link><link href="http://michaeljaylissner.com/feeds/tag/courtlistenercom" rel="self"></link><id>http://michaeljaylissner.com/</id><updated>2014-09-28T00:00:00-07:00</updated><entry><title>Updating Bulk Data in CourtListener</title><link href="http://michaeljaylissner.com/posts/2014/09/28/updating-bulk-data-in-courtlistener/" rel="alternate"></link><updated>2014-09-28T00:00:00-07:00</updated><author><name>Mike Lissner</name></author><id>tag:michaeljaylissner.com,2014-09-28:posts/2014/09/28/updating-bulk-data-in-courtlistener/</id><summary type="html">
&lt;p&gt;There’s an increasing demand for bulk data from government systems, and while this will generate big wins for transparency, accountability, and innovation (at the occasional cost of privacy&lt;sup id="fnref:privacy"&gt;&lt;a class="footnote-ref" href="#fn:privacy" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;), it’s important to consider a handful of technical difficulties that &lt;em&gt;can&lt;/em&gt; come along with creating and providing such data. Do &lt;em&gt;not&lt;/em&gt; misread this post as me saying, “bulk data is hard, don’t bother doing it.” Rather, like most of my posts, read this as an in-the-trenches account of issues we’ve encountered and solutions we’ve developed at CourtListener. &lt;/p&gt;
&lt;h2 id="the-past-and-present"&gt;The Past and Present&lt;/h2&gt;
&lt;p&gt;For the past several years we’ve had &lt;a href="https://www.courtlistener.com/api/bulk-info/"&gt;bulk data at CourtListener&lt;/a&gt;, but to be frank, it’s been pretty terrible in a lot of ways. Probably the biggest issue with it was that we created it as a single massive &lt;span class="caps"&gt;XML&lt;/span&gt; file (~&lt;span class="caps"&gt;13GB&lt;/span&gt;, compressed!). That made a lot of sense for our backend processing, but people consuming the bulk data complained that it crashed 32 bit systems&lt;sup id="fnref:sympathy"&gt;&lt;a class="footnote-ref" href="#fn:sympathy" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;, consumed memory endlessly, decompressing it wasn’t possible on Windows&lt;sup id="fnref:sympathy"&gt;&lt;a class="footnote-ref" href="#fn:sympathy" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;, etc. &lt;/p&gt;
&lt;p&gt;On top of these issues for people consuming our bulk data, and even though we set it up to be efficient for our servers, we did a stupid thing when we set it up and made it so our users could generate bulk files whenever they wanted for any day, month, year or jurisdiction. And create bulk files they did. Indeed in the year since we started keeping tabs on this, people made nearly fifty thousand requests for time-based bulk data&lt;sup id="fnref:stats"&gt;&lt;a class="footnote-ref" href="#fn:stats" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;
&lt;p&gt;On the backend, the way this worked was that the first time somebody wanted a bulk file, they requested it, we generated it, and then we served it. The second time somebody requested that same file, we just served the one we generated  before, creating a disk-based cache of sorts. This actually worked pretty well but it let people start long-running processes on our server that could degrade the performance of the front end. It wasn’t great, but it was a simple way to serve time- and jurisdiction-based files.&lt;sup id="fnref:file-count"&gt;&lt;a class="footnote-ref" href="#fn:file-count" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;
&lt;p&gt;As any seasoned developer knows, the next problem with such a system would be cache invalidation. How would we know that a cached bulk file had bad data and how would we delete it if necessary? Turns out this wasn’t so hard, but every time we changed (or deleted) an item in our database we had code that went out to the cache on disk and deleted any bulk files that might have changed data. Our data doesn’t change often, so for the most part this worked, but it’s the kind of spaghetti code you want to avoid. Touching disk whenever an item is updated? Not so good.  &lt;/p&gt;
&lt;p&gt;And there were bugs. &lt;a href="https://github.com/freelawproject/courtlistener/issues/278"&gt;Weird ones&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Yeah, the old system kind of sucked. The last few days I’ve been busy re-working our bulk data system to make it more reliable, easier to use and just overall, better.&lt;/p&gt;
&lt;h2 id="the-new-system-coming-soon"&gt;The New System (Coming Soon)&lt;/h2&gt;
&lt;p&gt;Let’s get the bad news taken care of off the bat: The new system no longer allows date-based bulk files. Since these could cause performance issues on the front end, and since &lt;a href="http://lists.freelawproject.org/pipermail/dev/2014-August/000069.html"&gt;nobody opposed the change&lt;/a&gt;, we’ve done away with this feature. It had a good life, may it &lt;span class="caps"&gt;RIP&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The good news is that by getting rid of the date-based bulk files, we’ve been able to eliminate a metric &lt;em&gt;ton&lt;/em&gt; of complexity, &lt;a href="http://theweek.com/article/index/241002/how-the-wrong-definition-of-literally-snuck-into-the-dictionary"&gt;literally&lt;/a&gt;! No longer do we need the disk-cache. No longer do we need to parse URLs and generate bulk data on the fly. No longer is the code a mess of decision trees based on cache state and user requests. Ah, it feels so free at last! &lt;/p&gt;
&lt;p&gt;And it gets even better. On top of this, we were able to resolve &lt;a href="https://github.com/freelawproject/courtlistener/issues/285"&gt;a long-standing feature request&lt;/a&gt; for complete bulk data files by jurisdiction. We were able to make the schema of the bulk files match that of &lt;a href="https://www.courtlistener.com/api/rest-info/"&gt;our &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt;. We were able to make the bulk file a tar of smaller &lt;span class="caps"&gt;JSON&lt;/span&gt; files, so no more issues unzipping massive files or having 32 bit systems crash. &lt;a href="https://www.youtube.com/watch?v=8vZx7yF_a7M"&gt;We settled all the family business&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Oh, and one more thing: When this goes live, we’ll have bulk files and an &lt;span class="caps"&gt;API&lt;/span&gt; for oral arguments as well — Another CourtListener first. &lt;/p&gt;
&lt;h2 id="jeez-thats-great-whyd-you-wait-so-long"&gt;Jeez, That’s Great, Why’d You Wait So Long?&lt;/h2&gt;
&lt;p&gt;This is a fair question. If it was possible to gain so much so quickly, why didn’t we do it sooner? Well, there are a number of reasons but at the core, like so many things, it’s because nothing is actually that easy. &lt;/p&gt;
&lt;p&gt;Before we could make these improvements, we needed to: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure the impact on our users &lt;a href="http://lists.freelawproject.org/pipermail/dev/2014-August/000069.html"&gt;wouldn’t be an issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Test the performance of generating more than 350 bulk files at the end of each month&lt;sup id="fnref:dev-aside"&gt;&lt;a class="footnote-ref" href="#fn:dev-aside" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Have &lt;a href="https://www.courtlistener.com/api/rest-info/"&gt;our &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; in place so we could use it to generate the bulk files&lt;/li&gt;
&lt;li&gt;Complete &lt;a href="https://github.com/freelawproject/courtlistener/commit/a0e4326d98e9f501ec3e69955d6b5650471686e8#diff-30d04f22c69dda9704be56ec95d9d2c1R68"&gt;performance profiling&lt;/a&gt; to identify &lt;a href="https://github.com/freelawproject/courtlistener/commit/a0e4326d98e9f501ec3e69955d6b5650471686e8#diff-6f850cf75fe2e1d17284e0b701b26b06L47"&gt;hot spots&lt;/a&gt; in the new bulk data &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/freelawproject/courtlistener/commit/52e8eff985fdf75612837cef4d9ef55ad60f29ad#diff-6"&gt;Rewrite the documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And pretty much everything else you can imagine. So, I suppose the answer is: We waited so long because it was hard. &lt;/p&gt;
&lt;p&gt;But being hard is one thing. Another thing is that although a number of organizations have used our bulk data, never has any contributed either energy or resources to fixing the bugs that they reported. Despite the benefits these organizations got from the bulk files, none chose to support the ecosystem from which they benefited. You can imagine how this isn’t particularly motivational for us, but we’re hopeful that with the new and improved system, those using our data will appreciate our efforts and consider &lt;a href="https://www.courtlistener.com/donate/"&gt;supporting us&lt;/a&gt; down the road.  &lt;/p&gt;
&lt;p&gt;So, without sucking on too many sour grapes, that’s the story behind the upgrades we’re making to the bulk files at CourtListener. At first blush it may seem like a fairly straightforward feature to get in place (and remember, in &lt;em&gt;many&lt;/em&gt; cases bulk data is stupid-easy to do), but we thought it would be interesting to share our experiences so others might compare notes. If you’re a consumer of CourtListener bulk data, we’ll be taking the wraps off of these new features soon, so make sure to watch the &lt;a href="http://freelawproject.org"&gt;Free Law Project blog&lt;/a&gt;. If you’re a developer that’s interested in this kind of thing, we’re eager to hear your feedback and any thoughts you might have. &lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:sympathy"&gt;
&lt;p&gt;I confess I’m not &lt;em&gt;that&lt;/em&gt; sympathetic… &lt;a class="footnote-backref" href="#fnref:sympathy" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:privacy"&gt;
&lt;p&gt;For example, a few days ago some folks got access to &lt;span class="caps"&gt;NYC&lt;/span&gt; taxi information in bulk. In theory it was anonymized using &lt;span class="caps"&gt;MD5&lt;/span&gt; hashing, but because there were a limited number of inputs into the hashing algorithm, all it took to de-anonymize the data was to compute every possible hash (“&lt;a href="https://medium.com/@vijayp/of-taxis-and-rainbows-f6bc289679a1"&gt;computing the 22M hashes took less than 2 minutes&lt;/a&gt;“) and then work backwards from there to the original IDs. While one researcher did that, another one began finding celebrities in taxis and figuring out where they went. Privacy is hard.  &lt;a class="footnote-backref" href="#fnref:privacy" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:stats"&gt;
&lt;p&gt;To be exact: 48271 requests, as gathered by our stats module. &lt;a class="footnote-backref" href="#fnref:stats" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:file-count"&gt;
&lt;p&gt;So far, 17866 files were created this way that haven’t been invalidated, as counted by: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;maxdepth&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;read&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;dir&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="n"&gt;printf&lt;/span&gt; &lt;span class="s"&gt;"%s:&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt; &lt;span class="s"&gt;"$dir"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="s"&gt;"$dir"&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;wc&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a class="footnote-backref" href="#fnref:file-count" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:dev-aside"&gt;
&lt;p&gt;A fun digression for the developers. As you might expect, aside from compressing bulk files, the bottleneck of generating 350+ bulk files at once is pulling items from the database and converting them to &lt;span class="caps"&gt;JSON&lt;/span&gt;. We tried a few solutions to this problem, but the best we came up with takes advantage of the fact that every item in the database belongs in exactly two bulk files: The all.tar.gz file and the {jurisdiction}.tar.gz file. One way to put the item into both places would be to generate the all.tar.gz file and then generate each of the 350 smaller files. &lt;/p&gt;
&lt;p&gt;That would iterate every item in the database twice, but while making the jurisdiction files you’d have to do a lot of database filtering…something that it’s generally good to avoid. Our solution to this problem is to create a dictionary of open file handles and then to iterate the entire database once. For each item in the database, add it to both the all.tar.gz file and add it to the {jurisdiction}.tar.gz file. Once complete, close all the file handles. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Get all the courts&lt;/span&gt;
&lt;span class="n"&gt;courts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;objects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c"&gt;# Create a dictionary with one key per jurisdiction&lt;/span&gt;
&lt;span class="n"&gt;tar_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;court&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;courts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s"&gt;'/tmp/bulk/opinions/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;.tar.gz'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'w:gz'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;compresslevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Then iterate over everything, adding it to the correct key&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;item_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Add the json str to the two tarballs&lt;/span&gt;
    &lt;span class="n"&gt;tarinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TarInfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;.json"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;docket&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;court_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tarinfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'all'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tarinfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In a sense the first part creates a variable for every jurisdiction on the fly and the second part uses that variable as a dumping point for each item as it iterates over them. &lt;/p&gt;
&lt;p&gt;A fine hack. &lt;a class="footnote-backref" href="#fnref:dev-aside" rev="footnote" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="bulk data"></category><category term="courtlistener.com"></category></entry></feed>